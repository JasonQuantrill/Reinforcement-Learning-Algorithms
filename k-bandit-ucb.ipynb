{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "random.seed(time.time_ns())\n",
    "\n",
    "class KBandit:\n",
    "    def __init__(self, k):\n",
    "        self.K = k\n",
    "        self.n = 0\n",
    "\n",
    "        self._initialize_qs()\n",
    "        self.reward_list = [0] * k\n",
    "        self.freqs = [0] * k\n",
    "\n",
    "    def _initialize_qs(self):\n",
    "        self.probs = []\n",
    "\n",
    "        for i in range(self.K):\n",
    "            self.probs.append(random.uniform(0, 1))\n",
    "\n",
    "    def _update_reward_estimates(self, action_index, new_rew):\n",
    "        if self.n == 0:\n",
    "            self.reward_list[action_index] = new_rew\n",
    "        else:\n",
    "            self.reward_list[action_index] = self.reward_list[action_index] + (new_rew - self.reward_list[action_index]) / self.n\n",
    "\n",
    "        self.n += 1\n",
    "\n",
    "    def action_reward(self, action_index):\n",
    "        rew = 1 if random.uniform(0, 1) < self.probs[action_index] else 0\n",
    "        \n",
    "        self._update_reward_estimates(action_index, rew)\n",
    "        self.freqs[action_index] += 1\n",
    "\n",
    "        return rew\n",
    "\n",
    "class Q1:\n",
    "    def __init__(self, log_freq=100, log=False):\n",
    "        self.log = log\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        self._init_tb()\n",
    "    \n",
    "    def _init_tb(self):\n",
    "        self.tb = KBandit(10)\n",
    "\n",
    "        self.current_reward = 0\n",
    "        self.rewards = []\n",
    "\n",
    "        self.num_optimal_actions = 0\n",
    "        self.optimal_percs = []\n",
    "\n",
    "    def _ucb_formula(self, t, qt, nt, C):\n",
    "        ln_t = math.log(t + np.e)\n",
    "        nt_a = nt + 1\n",
    "\n",
    "        return qt + C * math.log(ln_t /nt_a, 2)\n",
    "\n",
    "    def _ucb_action(self, t, qts, nts, C):\n",
    "        estimated_rewards = [self._ucb_formula(t, qt, nt, C) for qt, nt in zip(qts, nts)]\n",
    "\n",
    "        self.optimal_percs.append(self.num_optimal_actions * 100/ (t+1))\n",
    "        \n",
    "        if self.log and (t + 1) % self.log_freq == 0 :\n",
    "            print(f'Step: {t + 1}, average reward:  {self.optimal_percs[-1] / 100}')\n",
    "\n",
    "        return np.argmax(estimated_rewards)\n",
    "        \n",
    "\n",
    "    def run(self, episodes, exploration_coef=0.1, log=False):\n",
    "\n",
    "        self._init_tb()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            action_index = self._ucb_action(i, self.tb.reward_list, self.tb.freqs, exploration_coef)\n",
    "            new_rew = self.tb.action_reward(action_index)\n",
    "\n",
    "            self.current_reward += new_rew\n",
    "\n",
    "            if log and (i % self.log_freq == 0 or i == episodes - 1): \n",
    "                print(f'Episode {i}: %optimal: {self.optimal_percs[-1]}, avg reward: {self.current_reward / (i+1)}')\n",
    "            \n",
    "            if np.argmax(self.tb.probs) == action_index: self.num_optimal_actions += 1\n",
    "\n",
    "            self.rewards.append(self.current_reward / (i + 1))\n",
    "\n",
    "####### DEMONSTRATION\n",
    "\n",
    "CONFIDENCE_COEFFICIENTS = [0, 0.01, 0.05, 0.1, 0.25]\n",
    "STEPS = 5000\n",
    "RUNS = 1000\n",
    "\n",
    "def fetch_optimal_metric(coefs, steps):\n",
    "    plots = []\n",
    "    rewards = []\n",
    "    q = Q1()\n",
    "\n",
    "    for C in coefs:\n",
    "        q.run(steps, C)\n",
    "        rewards.append(q.rewards)\n",
    "        plots.append(q.optimal_percs)\n",
    "\n",
    "    return plots, rewards\n",
    "\n",
    "def plot_diff_coefs(plots, yLabel, lim=-1):\n",
    "    for P in plots:\n",
    "        plt.plot(P[:lim])\n",
    "\n",
    "    plt.legend([f'c = {C}' for C in CONFIDENCE_COEFFICIENTS], loc='lower right')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel(f'{yLabel}')\n",
    "    plt.show()\n",
    "\n",
    "def multiple_runs(n=10):\n",
    "    SUM_OPTIMAL = np.zeros((len(CONFIDENCE_COEFFICIENTS), STEPS))\n",
    "    SUM_REWARDS = np.zeros((len(CONFIDENCE_COEFFICIENTS), STEPS))\n",
    "    for i in range(n):\n",
    "        optimals, rewards = fetch_optimal_metric(CONFIDENCE_COEFFICIENTS, STEPS)\n",
    "        SUM_OPTIMAL += np.array(optimals)\n",
    "        SUM_REWARDS += np.array(rewards)\n",
    "\n",
    "    return SUM_OPTIMAL / n, SUM_REWARDS / n\n",
    "\n",
    "## Runs and logs for one run\n",
    "q = Q1()\n",
    "q.run(5000, 0.1, True)\n",
    "\n",
    "# Graphs the 50 runs\n",
    "opts, rews = multiple_runs(RUNS)\n",
    "plot_diff_coefs(opts, '% Optimal Actions')\n",
    "plot_diff_coefs(rews, 'Average Rewards')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
